{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üéôÔ∏è VoiceMorphAI ‚Äî Public Working Version (No HF login)\n",
    "# =========================================================\n",
    "\n",
    "!pip install -q torch torchvision torchaudio diffusers transformers accelerate gradio librosa soundfile soxr matplotlib scipy\n",
    "\n",
    "import gradio as gr\n",
    "import torch, librosa, soundfile as sf, numpy as np, io, matplotlib.pyplot as plt\n",
    "from diffusers import AudioLDM2Pipeline   # open public model\n",
    "\n",
    "# ---- Load a public audio-diffusion model (no token needed)\n",
    "model_id = \"cvssp/audioldm2-large\"  # ‚úÖ open-access model\n",
    "pipe = AudioLDM2Pipeline.from_pretrained(model_id)\n",
    "pipe = pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- Core conversion function\n",
    "def convert_voice(prompt, input_audio):\n",
    "    audio, sr = librosa.load(input_audio, sr=16000)\n",
    "    dur = librosa.get_duration(y=audio, sr=sr)\n",
    "    result = pipe(prompt=prompt, num_inference_steps=10, audio_length_in_s=dur).audios[0]\n",
    "    out_path = \"converted.wav\"\n",
    "    sf.write(out_path, result, 16000)\n",
    "\n",
    "    # spectrogram\n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    spec = librosa.amplitude_to_db(np.abs(librosa.stft(result)), ref=np.max)\n",
    "    img = librosa.display.specshow(spec, sr=16000, x_axis=\"time\", y_axis=\"log\", ax=ax)\n",
    "    plt.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "    plt.title(\"Generated Spectrogram\")\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    buf.seek(0)\n",
    "    return out_path, buf\n",
    "\n",
    "# ---- Launch Gradio UI\n",
    "gr.Interface(\n",
    "    fn=convert_voice,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Describe new voice style (e.g. calm female, robotic, whisper)\"),\n",
    "        gr.Audio(type=\"filepath\", label=\"Input voice (.wav)\")\n",
    "    ],\n",
    "    outputs=[gr.Audio(label=\"Generated Voice\"), gr.Image(label=\"Spectrogram\")],\n",
    "    title=\"üéµ VoiceMorphAI ‚Äî Diffusion Voice Conversion (Public Model)\",\n",
    "    description=\"Upload any voice sample and describe a target style. Generates new voice using AudioLDM2 diffusion model.\"\n",
    ").launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
